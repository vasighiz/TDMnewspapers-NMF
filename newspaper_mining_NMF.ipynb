{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6299107c",
   "metadata": {},
   "source": [
    "Topic Modeling is a general method that can be useful for newspaper text analysis. This method involves using machine learning algorithms to identify the underlying topics or themes present in a collection of newspaper articles. By clustering similar articles together based on their topic, researchers can gain a better understanding of the issues, debates, and trends that are shaping the news coverage.\n",
    "\n",
    "2.	Non-negative Matrix Factorization (NMF): NMF is a matrix factorization technique that decomposes a matrix of word frequencies into two matrices representing a set of topics and the corresponding weights of each topic for each document.\n",
    "\n",
    "In this code, we first load the text data from the folder containing the .txt files and then convert the text data into a document-term matrix using the TF-IDF vectorizer from Scikit-learn. We then create an NMF model with 10 topics and fit it to the document-term matrix.\n",
    "\n",
    "We print the top 10 words for each topic and assign a topic to each document based on the highest probability topic assignment. Finally, we print the top 5 documents for each topic to get an idea of the types of documents that fall under each topic. Note that in this case, we are printing the filenames of the documents instead of their content since we are working with a folder of .txt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03187367-7029-43f2-9dca-85ca659caf3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15108b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "import chardet\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cb814150-27d5-48e3-86e2-f9ef76c250b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from watermark import watermark\n",
    "\n",
    "# %load_ext watermark\n",
    "\n",
    "# %watermark --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12ea9735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the root folder containing all subfolders\n",
    "# root_path = 'C:\\\\textmining\\\\echo'\n",
    "root_path ='echo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99290e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to hold the text data\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d56a68c-5671-48ff-9c16-9e10dacea483",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_list = sorted(os.listdir(root_path))\n",
    "folder_paths = [os.path.join(root_path,i) for i in folder_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4944548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traverse the directory tree and read all text files\n",
    "for dirpath, dirnames, filenames in os.walk(root_path):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "                data.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76a33dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'echo\\\\1876_01\\\\1876-01-28-0004.txt'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3a1e342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import re\n",
    "\n",
    "# # Define the root directory containing the text files\n",
    "# root_dir = 'D:/textmining/echo'\n",
    "\n",
    "# # Define regular expression to match non-alphanumeric characters\n",
    "# regex = re.compile('[^a-zA-Z0-9\\s]')\n",
    "\n",
    "# # Initialize an empty set to store unique special characters found\n",
    "# unique_chars = set()\n",
    "\n",
    "# # Loop over all files in the root directory and its subdirectories\n",
    "# for subdir, dirs, files in os.walk(root_dir):\n",
    "#     for file in files:\n",
    "#         # Check if the file is a text file\n",
    "#         if file.endswith('.txt'):\n",
    "#             # Read in the text file\n",
    "#             file_path = os.path.join(subdir, file)\n",
    "#             with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#                 text = f.read()\n",
    "\n",
    "#             # Find all matches of the regular expression in the text\n",
    "#             matches = re.findall(regex, text)\n",
    "\n",
    "#             # Add any unique matches found to the set of unique special characters\n",
    "#             for match in matches:\n",
    "#                 unique_chars.add(match)\n",
    "\n",
    "# # Print the set of unique special characters found\n",
    "# if len(unique_chars) > 0:\n",
    "#     print('Unique special characters found:', unique_chars)\n",
    "# else:\n",
    "# # Define regular expression to match non-alphanumeric characters\n",
    "# regex = re.compile('[^a-zA-Z0-9\\s]')#     print('No special characters found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f00db8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define regular expression to match non-alphanumeric characters\n",
    "regex = re.compile('[^a-zA-Z0-9\\s]')\n",
    "\n",
    "# Loop over all files in the root directory and its subdirectories\n",
    "for subdir, dirs, files in os.walk(root_path):\n",
    "    for file in files:\n",
    "        # Check if the file is a text file\n",
    "        if file.endswith('.txt'):\n",
    "            # Read in the text file\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "\n",
    "            # Replace all matches of the regular expression in the text with an empty string\n",
    "            text = re.sub(regex, '', text)\n",
    "\n",
    "            # Write the modified text back to the file\n",
    "            with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6721013b",
   "metadata": {},
   "source": [
    "Before converting the text data to a document-term matrix, we try removing stop words and stemming/lemmatizing the text before passing it to the TfidfVectorizer to reduce the number of terms and improve the quality of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6ba252",
   "metadata": {},
   "source": [
    "we have set max_features=1000, which means that the vectorizer will only consider the top 1000 most frequent terms in the corpus. You can experiment with different values of max_features to find the best setting for your dataset and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1949381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words and convert text data to a document-term matrix\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=100000, min_df=2)\n",
    "dtm = tfidf.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e52d3628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an NMF model with k topics\n",
    "k = 5\n",
    "nmf_model = NMF(n_components=k, init='nndsvd', solver= 'mu', max_iter= 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b1ca961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prgrm\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1411: UserWarning: The multiplicative update ('mu') solver cannot update zeros present in the initialization, and so leads to poorer results when used jointly with init='nndsvd'. You may try init='nndsvda' or init='nndsvdar' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(init='nndsvd', max_iter=10000, n_components=5, solver='mu')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model to the document-term matrix\n",
    "nmf_model.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01d8ce66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic 1:\n",
      "['good', 'nnd', 'house', 'amherstburg', 'town', 'windsor', 'ho', 'mr', 'bo', 'tho']\n",
      "\n",
      "\n",
      "Top 10 words for topic 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prgrm\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['books', 'boots', 'old', 'cash', 'best', 'tweeds', 'new', 'hand', 'stock', 'goods']\n",
      "\n",
      "\n",
      "Top 10 words for topic 3:\n",
      "['moved', 'ont', 'motion', 'election', 'council', 'house', 'money', 'wigle', 'amherstburg', 'mr']\n",
      "\n",
      "\n",
      "Top 10 words for topic 4:\n",
      "['meeting', 'bank', 'amherstburg', 'malden', 'soap', 'esq', 'evening', '00', 'town', 'mr']\n",
      "\n",
      "\n",
      "Top 10 words for topic 5:\n",
      "['amherstburg', 'cured', 'cases', 'fever', 'remedy', 'hand', 'tho', 'cure', 'pain', 'charm']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print top 10 words for each topic\n",
    "for i, topic in enumerate(nmf_model.components_):\n",
    "    print(f'Top 10 words for topic {i+1}:')\n",
    "    print([tfidf.get_feature_names()[index] for index in topic.argsort()[-10:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68d75586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic assignments for each document\n",
    "topic_results = nmf_model.transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c252b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 documents for topic 1:\n",
      "[]\n",
      "\n",
      "\n",
      "Top 5 documents for topic 2:\n",
      "[]\n",
      "\n",
      "\n",
      "Top 5 documents for topic 3:\n",
      "[]\n",
      "\n",
      "\n",
      "Top 5 documents for topic 4:\n",
      "[]\n",
      "\n",
      "\n",
      "Top 5 documents for topic 5:\n",
      "[]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the top 5 documents for each topic\n",
    "for i in range(k):\n",
    "    print(f'Top 5 documents for topic {i+1}:')\n",
    "    file_names = [os.path.basename(files[j]) for j in range(len(files)) if topic_results[j, i] == max(topic_results[:, i])]\n",
    "    print(file_names[:5])\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0357e9a2-a585-4057-9936-9008d0d6190d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install watermark\n",
    "\n",
    "\n",
    "# from watermark import watermark\n",
    "\n",
    "# %watermark --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c302f974-37c6-4f08-9cb7-b8d0002e45c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

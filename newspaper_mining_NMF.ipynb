{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6299107c",
   "metadata": {},
   "source": [
    "Topic Modeling is a general method that can be useful for newspaper text analysis. This method involves using machine learning algorithms to identify the underlying topics or themes present in a collection of newspaper articles. By clustering similar articles together based on their topic, researchers can gain a better understanding of the issues, debates, and trends that are shaping the news coverage.\n",
    "\n",
    "Non-negative Matrix Factorization (NMF): NMF is a matrix factorization technique that decomposes a matrix of word frequencies into two matrices representing a set of topics and the corresponding weights of each topic for each document.\n",
    "\n",
    "In this code, we first load the text data from the folder containing the .txt files and then convert the text data into a document-term matrix using the TF-IDF vectorizer from Scikit-learn. We then create an NMF model with 10 topics and fit it to the document-term matrix.\n",
    "\n",
    "We print the top 10 words for each topic and assign a topic to each document based on the highest probability topic assignment. Finally, we print the top 5 documents for each topic to get an idea of the types of documents that fall under each topic. Note that in this case, we are printing the filenames of the documents instead of their content since we are working with a folder of .txt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0807d3-a1de-4f53-8460-8050708fabdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sklearn --no-index #only needed on SHARCNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15108b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import heapq\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "import chardet\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cb814150-27d5-48e3-86e2-f9ef76c250b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from watermark import watermark\n",
    "\n",
    "# %load_ext watermark\n",
    "\n",
    "# %watermark --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12ea9735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the root folder containing all subfolders\n",
    "# root_path = 'C:\\\\textmining\\\\echo'\n",
    "root_path ='challenge' + os.sep + 'echo'\n",
    "doc_type = 'volumes'\n",
    "min_word_len = 4 #try to avoid mr, sr, the, etc.\n",
    "news_range = '[1920 to 1930]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99290e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to hold the text data\n",
    "data = []\n",
    "# And something for files\n",
    "files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d56a68c-5671-48ff-9c16-9e10dacea483",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_list = sorted(os.listdir(root_path))\n",
    "folder_paths = [os.path.join(root_path,i) for i in folder_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4944548f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantity 571\n"
     ]
    }
   ],
   "source": [
    "ranges = re.findall(r'\\d+', news_range) #pick out numbers\n",
    "range0 = int(ranges[0])\n",
    "range1 = int(ranges[1])\n",
    "re_lns = re.compile(\"\\r\\n?|\\n\") #remove line returns\n",
    "re_alpha = re.compile('[^a-zA-Z\\s]') #remove non-alpha\n",
    "prob_words = ['mr','mrs','sald','th', 'amherstburg'] #non-topic words and common OCR errors\n",
    "\n",
    "docpath = os.sep + doc_type\n",
    "\n",
    "# Traverse the directory tree and read all text files\n",
    "for dirpath, dirnames, filenames in os.walk(root_path):\n",
    "    for filename in filenames:\n",
    "        folder = int(dirpath.split(os.sep)[2])\n",
    "        #use len(data) == 0 to debug:\n",
    "        if filename.endswith('.txt') and doc_type in dirpath and range0 <= folder <= range1:\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "                text = re.sub(re_lns, ' ', text)\n",
    "                text = re.sub(re_alpha, '', text)\n",
    "                text_words = text.split()\n",
    "                topic_words = [word for word in text_words if word.lower() not in prob_words and len(word) >= min_word_len]\n",
    "                data.append(' '.join(topic_words))\n",
    "                files.append(file_path)\n",
    "print(\"quantity\", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6721013b",
   "metadata": {},
   "source": [
    "Before converting the text data to a document-term matrix, we try removing stop words and stemming/lemmatizing the text before passing it to the TfidfVectorizer to reduce the number of terms and improve the quality of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6ba252",
   "metadata": {},
   "source": [
    "we have set max_features=1000, which means that the vectorizer will only consider the top 1000 most frequent terms in the corpus. You can experiment with different values of max_features to find the best setting for your dataset and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f1949381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words and convert text data to a document-term matrix\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=10000, min_df=2)\n",
    "dtm = tfidf.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e52d3628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an NMF model with k topics\n",
    "k = 5\n",
    "nmf_model = NMF(n_components=k, init='nndsvda', solver= 'mu', max_iter= 10000)  #nndsvd or nndsvda or nndsvdar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b1ca961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(init='nndsvda', max_iter=10000, n_components=5, solver='mu')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model to the document-term matrix\n",
    "nmf_model.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "01d8ce66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic 1:\n",
      "['home', 'sunday', 'held', 'county', 'school', 'miss', 'phone', 'essex', 'years', 'church']\n",
      "\n",
      "\n",
      "Top 10 words for topic 2:\n",
      "['bank', 'farm', 'sullivan', 'street', 'house', 'acres', 'sale', 'phone', 'good', 'apply']\n",
      "\n",
      "\n",
      "Top 10 words for topic 3:\n",
      "['motion', 'mayor', 'windsor', 'sandwich', 'essex', 'road', 'town', 'reeve', 'county', 'council']\n",
      "\n",
      "\n",
      "Top 10 words for topic 4:\n",
      "['evening', 'teams', 'club', 'pins', 'league', 'score', 'games', 'high', 'game', 'team']\n",
      "\n",
      "\n",
      "Top 10 words for topic 5:\n",
      "['elected', 'years', 'councillors', 'january', 'deputy', 'year', 'acclamation', 'reeve', 'december', 'christmas']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print top 10 words for each topic\n",
    "for i, topic in enumerate(nmf_model.components_):\n",
    "    print(f'Top 10 words for topic {i+1}:')\n",
    "    print([tfidf.get_feature_names_out()[index] for index in topic.argsort()[-10:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68d75586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic assignments for each document\n",
    "topic_results = nmf_model.transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9c252b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 documents for topic 1:\n",
      "['1925-06-19.txt', '1925-09-25.txt', '1926-02-12.txt', '1926-09-24.txt', '1927-07-08.txt']\n",
      "Top 5 documents for topic 2:\n",
      "['1920-04-23.txt', '1920-08-13.txt', '1920-08-20.txt', '1920-08-27.txt', '1922-04-28.txt']\n",
      "Top 5 documents for topic 3:\n",
      "['1928-07-27.txt', '1929-09-13.txt', '1930-05-23.txt', '1930-05-30.txt', '1930-06-27.txt']\n",
      "Top 5 documents for topic 4:\n",
      "['1928-03-02.txt', '1928-03-16.txt', '1928-04-20.txt', '1928-11-09.txt', '1928-11-16.txt']\n",
      "Top 5 documents for topic 5:\n",
      "['1924-12-26.txt', '1925-12-25.txt', '1927-12-23.txt', '1927-12-30.txt', '1928-12-21.txt']\n"
     ]
    }
   ],
   "source": [
    "# Print the top 5 documents for each topic\n",
    "for i in range(k):\n",
    "    print(f'Top 5 documents for topic {i+1}:')\n",
    "    #file_names = [os.path.basename(files[j]) for j in range(len(files)) if topic_results[j, i] == max(topic_results[:, i])]\n",
    "    file_names = [os.path.basename(files[j]) for j in range(len(files)) if topic_results[j, i] in heapq.nlargest(k,topic_results[:,i])]\n",
    "    print(file_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0357e9a2-a585-4057-9936-9008d0d6190d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install watermark\n",
    "\n",
    "\n",
    "# from watermark import watermark\n",
    "\n",
    "# %watermark --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c302f974-37c6-4f08-9cb7-b8d0002e45c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
